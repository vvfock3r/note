# kuberneteséƒ¨ç½²

## éƒ¨ç½²æ–¹å¼

æ–‡æ¡£ï¼š[https://kubernetes.io/docs/setup/production-environment/tools/](https://kubernetes.io/docs/setup/production-environment/tools/)

| éƒ¨ç½²æ–¹å¼   | å¤æ‚æ€§ | çµæ´»æ€§ | æè¿°                                           |
| ---------- | ------ | ------ | ---------------------------------------------- |
| Kubespray  | ç®€å•   | è‡ªå®šä¹‰ | åŸºäº`kubeadm`å’Œ`Ansible`æ¥éƒ¨ç½²                 |
| kubeadm    | é€‚ä¸­   | è‡ªå®šä¹‰ | `Kubeadm `æ˜¯ä¸€ä¸ªå¿«æ·æ­å»º`kubernetes`çš„å®‰è£…å·¥å…· |
| äºŒè¿›åˆ¶éƒ¨ç½² | æœ€å¤æ‚ | æœ€çµæ´» | æ¨èç”Ÿäº§ç¯å¢ƒä½¿ç”¨                               |



## ç³»ç»Ÿåˆå§‹åŒ–

### ï¼ˆ1ï¼‰æ›´æ–°ç³»ç»Ÿ

```bash
# æ›´æ–°ç³»ç»Ÿå¹¶é‡å¯
[root@localhost ~]# yum -y install epel-release
[root@localhost ~]# yum -y update && reboot

# æŸ¥çœ‹ç³»ç»Ÿç‰ˆæœ¬
[root@localhost ~]# cat /etc/redhat-release
CentOS Linux release 7.9.2009 (Core)
```

### ï¼ˆ2ï¼‰é…ç½®æ—¶åŒºï¼ˆå¯é€‰ï¼‰

```bash
# å…ˆæ£€æŸ¥ä¸€ä¸‹å½“å‰çš„æ—¶åŒºæ˜¯å¦æ­£ç¡®
[root@localhost ~]# timedatectl
      Local time: Fri 2022-08-19 15:02:02 CST
  Universal time: Fri 2022-08-19 07:02:02 UTC
        RTC time: Fri 2022-08-19 07:02:00
       Time zone: Asia/Shanghai (CST, +0800)
     NTP enabled: n/a
NTP synchronized: no
 RTC in local TZ: no
      DST active: n/a

# é…ç½®ä¸ºä¸œå…«åŒº
[root@localhost ~]# timedatectl set-timezone "Asia/Shanghai"
```

### ï¼ˆ3ï¼‰é…ç½®24å°æ—¶åˆ¶ï¼ˆå¯é€‰ï¼‰

:::tip

CentOSé»˜è®¤æƒ…å†µå°±æ˜¯24å°æ—¶åˆ¶ï¼Œå¯¹äºå…¶ä»–Linuxå‘è¡Œç‰ˆæ¯”å¦‚Ubuntuå¯èƒ½æ˜¯12å°æ—¶åˆ¶ï¼Œæ ¹æ®è‡ªå·±çš„å–œå¥½ä¿®æ”¹

:::

```bash
# æŸ¥çœ‹å½“å‰æ—¶é—´
root@ubuntu:~# date
Sun Aug 14 10:21:00 PM CST 2022  # PMä»£è¡¨ä¸‹åˆ, å³æ™šä¸Šçš„10ç‚¹

# ä¿®æ”¹ä¸º24å°æ—¶åˆ¶
root@ubuntu:~# vim /etc/default/locale
LANG=en_US.UTF-8
LC_TIME=en_DK.UTF-8		# æ–°å¢è¿™ä¸€è¡Œ

# é‡å¯ç³»ç»Ÿï¼Œç„¶åå†æ¬¡æŸ¥çœ‹æ—¶é—´
root@ubuntu:~# date
Sun Aug 14 22:23:33 CST 2022  # å·²ä¿®æ”¹ä¸º22ç‚¹
```

### ï¼ˆ4ï¼‰é…ç½®é™æ€IPï¼ˆå¯é€‰ï¼‰

:::tip

å¦‚æœä½¿ç”¨`VMware Workstation`ç­‰åœ¨æœ¬åœ°éƒ¨ç½²ï¼Œéœ€è¦ä¿è¯ä½¿ç”¨é™æ€å†…ç½‘IPåœ°å€

:::

```bash
[root@localhost ~]# vi /etc/sysconfig/network-scripts/ifcfg-ens33
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
BOOTPROTO="static"		# è®¾ç½®ä¸ºé™æ€IP
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="068dc849-6e8c-4bed-b2de-2fe66c424521"
DEVICE="ens33"
ONBOOT="yes"			# å¼€å¯è‡ªå¯
IPADDR=192.168.48.140	# IPï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
NETMASK=255.255.255.0	# å­ç½‘æ©ç 
GATEWAY=192.168.48.2	# é»˜è®¤ç½‘å…³ï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
DNS1=192.168.48.2       # DNS1
DNS2=8.8.8.8            # DNS2

# é‡å¯ç½‘ç»œ
[root@localhost ~]# systemctl restart network.service

# æµ‹è¯•ç½‘ç»œ
[root@localhost ~]# ping -c 4 www.baidu.com
PING www.a.shifen.com (39.156.66.14) 56(84) bytes of data.
64 bytes from 39.156.66.14 (39.156.66.14): icmp_seq=1 ttl=128 time=27.3 ms
64 bytes from 39.156.66.14 (39.156.66.14): icmp_seq=2 ttl=128 time=28.0 ms
64 bytes from 39.156.66.14 (39.156.66.14): icmp_seq=3 ttl=128 time=43.1 ms
64 bytes from 39.156.66.14 (39.156.66.14): icmp_seq=4 ttl=128 time=23.9 ms

--- www.a.shifen.com ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3005ms
rtt min/avg/max/mdev = 23.975/30.612/43.163/7.406 ms
```

### ï¼ˆ5ï¼‰åŒæ­¥æœåŠ¡å™¨æ—¶é—´ï¼ˆå¯é€‰ï¼‰

```bash
[root@localhost ~]# yum install ntpdate -y
[root@localhost ~]# ntpdate time.windows.com
[root@localhost ~]# crontab -e
* * * * * /usr/sbin/ntpdate time.windows.com
```

### ï¼ˆ6ï¼‰é…ç½®ä¸»æœºå

```bash
# é…ç½®ä¸»æœºå
[root@localhost ~]# hostnamectl set-hostname node-1
[root@localhost ~]# hostnamectl set-hostname node-2
[root@localhost ~]# hostnamectl set-hostname node-3

# æ·»åŠ ä¸»æœºåè§£æ
[root@localhost ~]# cat >> /etc/hosts <<EOF

# kubernetes
192.168.48.142 node-1
192.168.48.143 node-2
192.168.48.144 node-3
EOF
```

### ï¼ˆ7ï¼‰å…³é—­æŸäº›æœåŠ¡

```bash
# å…³é—­é˜²ç«å¢™
[root@localhost ~]# systemctl stop firewalld && systemctl disable firewalld

# å…³é—­selinux
[root@localhost ~]# setenforce 0 && \
	getenforce && \
	sed -ri 's/(^SELINUX=)(.*)/\1disabled/' /etc/selinux/config && \
	grep -E '^SELINUX=' /etc/selinux/config

# è®¾ç½®iptablesè§„åˆ™
[root@localhost ~]# iptables -F && \
	iptables -X && \
	iptables -F -t nat && \
	iptables -X -t nat && \
	iptables -P FORWARD ACCEPT

# å…³é—­swap
[root@localhost ~]# swapoff -a && free
[root@localhost ~]# sed -ri '/(^[^#])(.*)[[:blank:]]swap[[:blank:]](.*)/s/^/#/' /etc/fstab && \
                    grep swap /etc/fstab

# å…³é—­dnsmasq(å¦åˆ™å¯èƒ½å¯¼è‡´å®¹å™¨æ— æ³•è§£æåŸŸå)
[root@localhost ~]# service dnsmasq stop && systemctl disable dnsmasq
```

### ï¼ˆ8ï¼‰è°ƒæ•´å†…æ ¸å‚æ•°

::: tip 

è‹¥å‡ºç°å¦‚ä¸‹æŠ¥é”™

```bash
[root@localhost ~]# sysctl -p /etc/sysctl.d/kubernetes.conf
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
net.ipv4.ip_nonlocal_bind = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
vm.overcommit_memory = 1
```

è§£å†³åŠæ³•

```bash
# æ£€æŸ¥æ¨¡å—æ˜¯å¦å·²ç»åŠ è½½ï¼ˆè¾“å‡ºä¸ºç©ºä»£è¡¨æ¨¡å—æ²¡æœ‰åŠ è½½ï¼‰
[root@node0 ~]# lsmod | grep br_netfilter
br_netfilter           22256  0 
bridge                151336  1 br_netfilter

# ä¸´æ—¶åŠ è½½æ¨¡å—(é‡å¯åè¿˜éœ€è¦é‡æ–°åŠ è½½)
[root@localhost ~]# modprobe br_netfilter

# è®¾ç½®å¼€å¯è‡ªåŠ è½½æ¨¡å—
[root@localhost ~]# echo br_netfilter > /etc/modules-load.d/br_netfilter.conf
```

:::

```bash
[root@localhost ~]# cat > /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_nonlocal_bind = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
vm.overcommit_memory = 1
EOF

[root@localhost ~]# sysctl -p /etc/sysctl.d/kubernetes.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_nonlocal_bind = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
vm.overcommit_memory = 1
```

### ï¼ˆ9ï¼‰å®‰è£…å¸¸ç”¨è½¯ä»¶åŒ…

```bash
[root@localhost ~]# yum -y install yum-utils \
	vim curl wget rsync \
	socat conntrack ipvsadm ipset \
	sysstat iptables libseccomp
```

### ï¼ˆ10ï¼‰è°ƒæ•´ulimit

```bash
# æ£€æŸ¥å½“å‰é…ç½®
[root@node-1 ~]# ulimit -a | grep -E 'open files|max user processes'
open files                      (-n) 1024
max user processes              (-u) 7184

# ä¸´æ—¶è®¾ç½®
[root@localhost ~]# ulimit -n 102400 && ulimit -u 102400

# æ°¸ä¹…è®¾ç½®
[root@localhost ~]# cat >>/etc/security/limits.conf <<EOF
# max number of open file descriptors
* soft nofile 102400
* hard nofile 102400

# max number of processes
* soft nproc  102400
* hard nproc  102400
EOF
```

### ï¼ˆ11ï¼‰é‡å¯ç³»ç»Ÿå†æ¬¡æ£€æŸ¥

## 

## ğŸ ä½¿ç”¨kubesprayéƒ¨ç½²

æ–‡æ¡£1ï¼š[https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubespray/)

æ–‡æ¡£2ï¼š[https://github.com/kubernetes-sigs/kubespray](https://github.com/kubernetes-sigs/kubespray)

### å¿…è¯»è¯´æ˜

**ï¼ˆ1ï¼‰èŠ‚ç‚¹è§„åˆ’**

:::tip

æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å®‰è£…æ“ä½œç³»ç»Ÿï¼Œå®‰è£…å®Œæˆåä¸éœ€è¦åšä»»ä½•æ“ä½œ

:::

| ä¸»æœºå | MasterèŠ‚ç‚¹ | NodeèŠ‚ç‚¹ | EtcdèŠ‚ç‚¹ | å…¶ä»–èŠ‚ç‚¹        | å†…å­˜ | CPU  | é™æ€IP         |
| ------ | ---------- | -------- | -------- | --------------- | ---- | ---- | -------------- |
| node0  | âœ”          | âœ”        | âœ”        | Ansibleä¸»æ§èŠ‚ç‚¹ | 4G   | 2æ ¸  | 192.168.48.128 |
| node1  | âœ”          | âœ”        | âœ”        |                 | 4G   | 2æ ¸  | 192.168.48.134 |
| node2  |            | âœ”        | âœ”        |                 | 4G   | 2æ ¸  | 192.168.48.135 |

**ï¼ˆ2ï¼‰ç‰ˆæœ¬è¯´æ˜**

| åç§°       |         ç‰ˆæœ¬ | å¤‡æ³¨                                       |
| ---------- | -----------: | ------------------------------------------ |
| OS         | `Centos 7.9` | æ‰€ä½¿ç”¨é•œåƒä¸º`CentOS-7-x86_64-DVD-1708.iso` |
| kubespray  |    `v2.19.0` |                                            |
| Kubernetes |    `v1.23.7` | `kubespray v2.19.0`é»˜è®¤å®‰è£…ç‰ˆæœ¬            |

**ï¼ˆ3ï¼‰ç§‘å­¦ä¸Šç½‘**

åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­éœ€è¦å»æµ·å¤–ä¸‹è½½é•œåƒï¼Œéœ€è¦ä¸»æœºèƒ½å¤Ÿç§‘å­¦ä¸Šç½‘ï¼ˆç›´è¿æˆ–è€…é€šè¿‡`HTTP_PROXY`æ–¹å¼ï¼‰

**ï¼ˆ4ï¼‰æœ€ä½é…ç½®**

æ”¯æŒä¸»æµç³»ç»Ÿï¼Œå†…å­˜æœ€ä½2Gï¼ŒCPUæœ€ä½2æ ¸ï¼Œç£ç›˜30Gä»¥ä¸Š

### é…ç½®Ansibleä¸»æ§èŠ‚ç‚¹

Ansibleä¸»æ§èŠ‚ç‚¹éƒ¨ç½²åœ¨å“ªé‡Œéƒ½å¯ä»¥ï¼Œåªè¦èƒ½æ§åˆ¶K8s NodeèŠ‚ç‚¹å³å¯

```bash
# ç”Ÿæˆkeygenï¼ˆæ‰§è¡Œssh-keygenï¼Œä¸€è·¯å›è½¦ä¸‹å»ï¼‰
[root@localhost ~]# ssh-keygen

# é…ç½®SSHå…å¯†ç™»å½•
[root@localhost ~]# ssh-copy-id root@192.168.48.128
[root@localhost ~]# ssh-copy-id root@192.168.48.134
[root@localhost ~]# ssh-copy-id root@192.168.48.135

# éªŒè¯å…å¯†ç™»å½•
[root@localhost ~]# ssh root@192.168.48.128  "hostname"
[root@localhost ~]# ssh root@192.168.48.134  "hostname"
[root@localhost ~]# ssh root@192.168.48.135  "hostname"

# å®‰è£…åŸºç¡€è½¯ä»¶
[root@localhost ~]# yum install epel-release python3 git wget -y
[root@localhost ~]# python3 --version
Python 3.6.8

# å‡çº§pipåˆ°æœ€æ–°ç‰ˆ(å¯é€‰,æ¨è)
[root@localhost ~]# pip3 install --upgrade pip -i https://mirrors.aliyun.com/pypi/simple

# ä¸‹è½½kubesprayæºç 
# è‹¥å› ç½‘ç»œä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥ä½¿ç”¨æˆ‘ä»¬å‡†å¤‡å¥½çš„ä»£ç†ï¼ˆç§‘å­¦ä¸Šç½‘ï¼‰ï¼Œwget -cåé¢æ·»åŠ  -e "http_proxy=http://192.168.5.103:7890"
[root@localhost ~]# wget -c https://github.com/kubernetes-sigs/kubespray/archive/v2.19.0.tar.gz 
[root@localhost ~]# tar zxf v2.19.0.tar.gz && cd kubespray-2.19.0

# å®‰è£…Pythonä¾èµ–ï¼ˆå› ä¸ºæˆ‘ä»¬æ˜¯Python 3.6.8ï¼Œæ‰€ä»¥è¿™é‡Œè¦ä½¿ç”¨requirements-2.11.txtï¼Œè¯¦ç»†ä¿¡æ¯å‚è€ƒGitHubï¼‰
[root@localhost kubespray-2.19.0]# pip3 install -r requirements-2.11.txt -i https://mirrors.aliyun.com/pypi/simple

# ç”Ÿæˆé¡¹ç›®é…ç½®
[root@localhost kubespray-2.19.0]# cp -rpf inventory/sample inventory/mycluster

# ä½¿ç”¨çœŸå®çš„hostnameï¼ˆå¦åˆ™ä¼šè‡ªåŠ¨æŠŠä½ çš„hostnameæ”¹æˆnode1/node2...è¿™ç§å“¦ï¼‰
[root@localhost kubespray-2.19.0]# export USE_REAL_HOSTNAME=true

# æŒ‡å®šé…ç½®æ–‡ä»¶ä½ç½®
[root@localhost kubespray-2.19.0]# export CONFIG_FILE=inventory/mycluster/hosts.yaml

# å®šä¹‰ipåˆ—è¡¨ï¼ˆä½ çš„æœåŠ¡å™¨å†…ç½‘ipåœ°å€åˆ—è¡¨ï¼Œ3å°åŠä»¥ä¸Šï¼Œå‰ä¸¤å°é»˜è®¤ä¸ºmasterèŠ‚ç‚¹ï¼‰
[root@localhost kubespray-2.19.0]# declare -a IPS=(
  192.168.48.128
  192.168.48.134
  192.168.48.135
)

# ç”Ÿæˆé…ç½®æ–‡ä»¶
[root@localhost kubespray-2.19.0]# python3 contrib/inventory_builder/inventory.py ${IPS[@]}
DEBUG: Adding group all
DEBUG: Adding group kube-master
DEBUG: Adding group kube-node
DEBUG: Adding group etcd
DEBUG: Adding group k8s-cluster
DEBUG: Adding group calico-rr
DEBUG: adding host node0 to group all
DEBUG: adding host node1 to group all
DEBUG: adding host node2 to group all
DEBUG: adding host node0 to group etcd
DEBUG: adding host node1 to group etcd
DEBUG: adding host node2 to group etcd
DEBUG: adding host node0 to group kube-master
DEBUG: adding host node1 to group kube-master
DEBUG: adding host node0 to group kube-node
DEBUG: adding host node1 to group kube-node
DEBUG: adding host node2 to group kube-node
```

### èŠ‚ç‚¹ä¸ªæ€§åŒ–é…ç½®

```bash
# å®šåˆ¶åŒ–é…ç½®æ–‡ä»¶
# 1. èŠ‚ç‚¹ç»„ç»‡é…ç½®ï¼ˆè¿™é‡Œå¯ä»¥è°ƒæ•´æ¯ä¸ªèŠ‚ç‚¹çš„è§’è‰²ï¼‰
[root@localhost kubespray-2.19.0]# cat inventory/mycluster/hosts.yaml
all:
  hosts:
    node0:
      ansible_host: 192.168.48.128
      ip: 192.168.48.128
      access_ip: 192.168.48.128
    node1:
      ansible_host: 192.168.48.134
      ip: 192.168.48.134
      access_ip: 192.168.48.134
    node2:
      ansible_host: 192.168.48.135
      ip: 192.168.48.135
      access_ip: 192.168.48.135
  children:
    kube_control_plane:
      hosts:
        node0:
        node1:
    kube_node:
      hosts:
        node0:
        node1:
        node2:
    etcd:
      hosts:
        node0:
        node1:
        node2:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}
      
# 2. containerdé…ç½®ï¼ˆè‡ªv2.18.0å¼€å§‹é»˜è®¤ä½¿ç”¨containerdä½œä¸ºå®¹å™¨è¿è¡Œæ—¶ï¼‰
[root@localhost kubespray-2.19.0]# vi inventory/mycluster/group_vars/all/containerd.yml

# 3. å…¨å±€é…ç½®ï¼ˆå¯ä»¥åœ¨è¿™é…ç½®http(s)ä»£ç†å®ç°å¤–ç½‘è®¿é—®ï¼‰
[root@localhost kubespray-2.19.0]# vi inventory/mycluster/group_vars/all/all.yml
http_proxy: "http://192.168.0.100:7890"     # é…ç½®ä»£ç†
https_proxy: "http://192.168.0.100:7890"    # é…ç½®ä»£ç†


# 4. k8sé›†ç¾¤é…ç½®ï¼ˆåŒ…æ‹¬è®¾ç½®å®¹å™¨è¿è¡Œæ—¶ã€svcç½‘æ®µã€podç½‘æ®µç­‰ï¼‰
[root@localhost kubespray-2.19.0]# vi inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
kube_version: v1.23.7                  # K8Sç‰ˆæœ¬ä¿¡æ¯ï¼Œæ— éœ€ä¿®æ”¹ï¼ˆä¹Ÿä¸è¦éšæ„ä¿®æ”¹ï¼‰
kube_service_addresses: 10.200.0.0/16  # é»˜è®¤ä¸º10.233.0.0/18ï¼Œä¿®æ”¹ä¸º10.200.0.0/16
kube_pods_subnet: 10.233.0.0/16        # é»˜è®¤10.233.64.0/18ï¼Œä¿®æ”¹ä¸º10.233.0.0/16
container_manager: containerd	       # é…ç½®å®¹å™¨å¼•æ“ï¼Œä¸ç”¨ä¿®æ”¹

# 5. ä¿®æ”¹etcdéƒ¨ç½²ç±»å‹ä¸ºhostï¼ˆé»˜è®¤æ˜¯dockerï¼‰
[root@localhost kubespray-2.19.0]# vi inventory/mycluster/group_vars/etcd.yml
etcd_deployment_type: host      # é…ç½®etcdéƒ¨ç½²æ–¹å¼ï¼Œé»˜è®¤æ˜¯dockerï¼Œå¦‚æœä½¿ç”¨containerdçš„è¯ï¼Œå¿…é¡»ä½¿ç”¨å®¿ä¸»æœºéƒ¨ç½²ï¼Œå³host

# 6. é™„åŠ ç»„ä»¶ï¼ˆingressã€dashboardç­‰ï¼‰
[root@localhost kubespray-2.19.0]# vi inventory/mycluster/group_vars/k8s_cluster/addons.yml
dashboard_enabled: true			# ä¿®æ”¹ä¸ºtrue
ingress_nginx_enabled: true		# ä¿®æ”¹ä¸ºtrue
metrics_server_enabled: true    # ä¿®æ”¹ä¸ºtrue
```

### éƒ¨ç½²Kubernetesé›†ç¾¤

```bash
# ä½¿ç”¨tmux(å¯é€‰)
[root@localhost kubespray-2.19.0]# yum install tmux -y
[root@localhost kubespray-2.19.0]# tmux new -s k8s_install

# éƒ¨ç½²Kubernetesé›†ç¾¤ï¼ˆè¿™ä¸€æ­¥æ‰§è¡Œçš„æ—¶é—´å¯èƒ½ä¼šå¾ˆé•¿ï¼Œè¿™é‡Œæˆ‘ä½¿ç”¨timeå‘½ä»¤æ¥ç»Ÿè®¡ä¸€ä¸‹æ—¶é•¿ï¼‰
# å¦‚æœæƒ³æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯æˆ–å®šä½å‡ºé”™çš„taskï¼Œå¯ä»¥æ·»åŠ -vvvv
[root@localhost kubespray-2.19.0]# time ansible-playbook -i inventory/mycluster/hosts.yaml  -b cluster.yml

real    29m43.274s
user    8m22.444s
sys     3m51.848s
```

> å®‰è£…æ­¥éª¤æ‰§è¡Œæ—¶é•¿å¹¶ä¸ç¨³å®šï¼Œæ ¹æ®ç³»ç»Ÿé…ç½®ã€ç½‘ç»œè´¨é‡è€Œä¸åŒï¼Œå¿«åˆ™åŠå°æ—¶ï¼Œæ…¢åˆ™å‡ ä¸ªå°æ—¶



### æ£€æŸ¥é›†ç¾¤çŠ¶æ€

```bash
# æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€(MasterèŠ‚ç‚¹æ‰§è¡Œ)
[root@localhost kubespray-2.19.0]# kubectl get node
NAME    STATUS   ROLES                  AGE   VERSION
node0   Ready    control-plane,master   15m   v1.23.7
node1   Ready    control-plane,master   14m   v1.23.7
node2   Ready    <none>                 13m   v1.23.7

# æŸ¥çœ‹MasterèŠ‚ç‚¹ç»„ä»¶çŠ¶æ€(MasterèŠ‚ç‚¹æ‰§è¡Œ)
[root@localhost kubespray-2.19.0]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
controller-manager   Healthy   ok                              
scheduler            Healthy   ok                              
etcd-2               Healthy   {"health":"true","reason":""}   
etcd-0               Healthy   {"health":"true","reason":""}   
etcd-1               Healthy   {"health":"true","reason":""}   

# æŸ¥çœ‹PodçŠ¶æ€
[root@localhost kubespray-2.19.0]# kubectl get pods -A
NAMESPACE       NAME                                          READY   STATUS    RESTARTS   AGE
ingress-nginx   ingress-nginx-controller-hs8ld                1/1     Running   0          13m
ingress-nginx   ingress-nginx-controller-k6qzm                1/1     Running   0          13m
ingress-nginx   ingress-nginx-controller-kcggb                1/1     Running   0          13m
kube-system     calico-kube-controllers-6dd874f784-wxf8q      1/1     Running   0          13m
kube-system     calico-node-krtfp                             1/1     Running   0          14m
kube-system     calico-node-sn44p                             1/1     Running   0          14m
kube-system     calico-node-vfzzd                             1/1     Running   0          14m
kube-system     coredns-76b4fb4578-5jkmj                      1/1     Running   0          12m
kube-system     coredns-76b4fb4578-75bdd                      1/1     Running   0          13m
kube-system     dns-autoscaler-7979fb6659-5597v               1/1     Running   0          12m
kube-system     kube-apiserver-node0                          1/1     Running   1          16m
kube-system     kube-apiserver-node1                          1/1     Running   1          15m
kube-system     kube-controller-manager-node0                 1/1     Running   1          16m
kube-system     kube-controller-manager-node1                 1/1     Running   1          15m
kube-system     kube-proxy-fh2wf                              1/1     Running   0          14m
kube-system     kube-proxy-znqrr                              1/1     Running   0          14m
kube-system     kube-proxy-znvz6                              1/1     Running   0          14m
kube-system     kube-scheduler-node0                          1/1     Running   1          16m
kube-system     kube-scheduler-node1                          1/1     Running   1          15m
kube-system     kubernetes-dashboard-584bfbb648-6k96s         1/1     Running   0          12m
kube-system     kubernetes-metrics-scraper-5dc755864d-glpwt   1/1     Running   0          12m
kube-system     metrics-server-749474f899-szbn5               1/1     Running   0          12m
kube-system     nginx-proxy-node2                             1/1     Running   0          14m
kube-system     nodelocaldns-cmzbt                            1/1     Running   0          12m
kube-system     nodelocaldns-gkgh9                            1/1     Running   0          12m
kube-system     nodelocaldns-m2zvj                            1/1     Running   0          12m
```

### æ¸…ç†ä»£ç†è®¾ç½®

```bash
# æ¸…ç†Containerd HTTPä»£ç†
[root@localhost ~]# cat /etc/systemd/system/containerd.service.d/http-proxy.conf
[Service]
Environment="HTTP_PROXY=http://192.168.0.100:7890" "HTTPS_PROXY=http://192.168.0.100:7890" "NO_PROXY=192.168.48.128,node0,node0.cluster.local,192.168.48.134,node1,node1.cluster.local,192.168.48.135,node2,node2.cluster.local,127.0.0.1,localhost,10.200.0.0/16,10.233.0.0/16,svc,svc.cluster.local"

[root@localhost ~]# mv /etc/systemd/system/containerd.service.d/http-proxy.conf /etc/systemd/system/containerd.service.d/http-proxy.conf_$(date +"%Y-%m-%d-%H%M%S")
[root@localhost ~]# systemctl daemon-reload
[root@localhost ~]# systemctl restart containerd

# æ¸…ç†Yum HTTPä»£ç†(æŠŠgrepå‡ºæ¥çš„ä»£ç†é…ç½®æ³¨é‡Šæˆ–åˆ é™¤å³å¯)
[root@localhost ~]# grep 7890 -r /etc/yum*
/etc/yum.conf:proxy=http://192.168.0.100:7890
```



### è®¿é—®dashboard

```bash
# åˆ›å»ºservice
[root@localhost ~]# cat > dashboard-svc.yaml <<EOF
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: dashboard
  labels:
    app: dashboard
spec:
  type: NodePort
  selector:
    k8s-app: kubernetes-dashboard
  ports:
  - name: https
    nodePort: 30000
    port: 443
    targetPort: 8443
EOF

[root@localhost ~]# kubectl apply -f dashboard-svc.yaml

# è®¿é—®dashboard
ä¸ºäº†é›†ç¾¤å®‰å…¨ï¼Œä» 1.7 å¼€å§‹ï¼Œdashboard åªå…è®¸é€šè¿‡ https è®¿é—®ï¼Œæˆ‘ä»¬ä½¿ç”¨nodeportçš„æ–¹å¼æš´éœ²æœåŠ¡ï¼Œå¯ä»¥ä½¿ç”¨ https://NodeIP:NodePort åœ°å€è®¿é—® 
å…³äºè‡ªå®šä¹‰è¯ä¹¦ é»˜è®¤dashboardçš„è¯ä¹¦æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œè‚¯å®šæ˜¯éå®‰å…¨çš„è¯ä¹¦ï¼Œå¦‚æœå¤§å®¶æœ‰åŸŸåå’Œå¯¹åº”çš„å®‰å…¨è¯ä¹¦å¯ä»¥è‡ªå·±æ›¿æ¢æ‰ã€‚ä½¿ç”¨å®‰å…¨çš„åŸŸåæ–¹å¼è®¿é—®dashboardã€‚ 
åœ¨dashboard-all.yamlä¸­å¢åŠ dashboardå¯åŠ¨å‚æ•°ï¼Œå¯ä»¥æŒ‡å®šè¯ä¹¦æ–‡ä»¶ï¼Œå…¶ä¸­è¯ä¹¦æ–‡ä»¶æ˜¯é€šè¿‡secretæ³¨è¿›æ¥çš„ã€‚
- â€“tls-cert-file
- dashboard.cer
- â€“tls-key-file
- dashboard.key

# åˆ›å»ºservice account
[root@localhost ~]# kubectl create sa dashboard-admin -n kube-system

# åˆ›å»ºè§’è‰²ç»‘å®šå…³ç³»
[root@localhost ~]# kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin

# æŸ¥çœ‹dashboard-adminçš„secretåå­—
[root@localhost ~]# ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk '{print $1}')

# æ‰“å°secretçš„token
[root@localhost ~]# kubectl describe secret -n kube-system ${ADMIN_SECRET} | grep -E '^token' | awk '{print $2}'

# æµè§ˆå™¨è®¿é—®
[root@localhost ~]# https://192.168.48.128:30000/
```



### FAQ

**ï¼ˆ1ï¼‰Download file error**

![image-20211229101545405](https://tuchuang-1257805459.cos.accelerate.myqcloud.com/image-20211229101545405.png)

> ä¸‹è½½æ–‡ä»¶å‡ºé”™ï¼Œä»ä»¥ä¸‹æ–¹é¢æ’æŸ¥
>
> * æ£€æŸ¥æœ¬åœ°ç½‘ç»œã€ä»£ç†æœåŠ¡å™¨æ˜¯å¦æ­£å¸¸
> * æ£€æŸ¥é…ç½®æ˜¯å¦å†™é”™
>   * æ¯”å¦‚å°†ä»£ç†æœåŠ¡å™¨çš„`http://`è¯¯å†™æˆäº†`https://`
>   * æ¯”å¦‚å°†ä»£ç†æœåŠ¡å™¨çš„`http://`è¯¯å†™æˆ`http:/`
>

<br />

**ï¼ˆ2ï¼‰ç»„ä»¶çŠ¶æ€ä¸ºUnhealthy**

`scheduler`å’Œ`controller-manager`ç»„ä»¶çŠ¶æ€ä¸º`Unhealthy`

```bash
# ä¿®å¤Unhealthy(åœ¨æ‰€æœ‰Masterä¸Šæ“ä½œ)
[root@localhost ~]# vi /etc/kubernetes/manifests/kube-controller-manager.yaml
    # - --port=0    # å°†è¿™ä¸€è¡Œæ³¨é‡Šæ‰
[root@localhost ~]# vi /etc/kubernetes/manifests/kube-scheduler.yaml
    # - --port=0    # å°†è¿™ä¸€è¡Œæ³¨é‡Šæ‰

# é‡å¯kubelet
[root@localhost ~]# systemctl restart kubelet
```

<br />

**ï¼ˆ3ï¼‰SSHè¶…æ—¶**

**é”™è¯¯æè¿°**

`Ansible`è¿æ¥æŠ¥é”™ä¿¡æ¯ï¼š`Timeout (12s) waiting for privilege escalation prompt`

æ‰‹åŠ¨è°ƒç”¨`ssh`å‘½ä»¤åˆ™ä¼šä¸€ç›´å¡ç€

**è§£å†³åŠæ³•**

æ–¹æ³•1ï¼šå…³é—­SSHåå‘è§£æï¼ˆæ¨èä½¿ç”¨ï¼‰

```bash
[root@node0 kubespray-2.19.0]# vi /etc/ssh/sshd_config 
UseDNS no

[root@node0 ~]# systemctl restart sshd.service
```

æ–¹æ³•2ï¼šè°ƒæ•´Ansible SSHè¶…æ—¶æ—¶é—´

```bash
# è°ƒå¤§è¶…æ—¶æ—¶é—´
[root@node0 kubespray-2.19.0]# vim ansible.cfg 
[ssh_connection]
# ...
timeout = 300			# è®¾ç½®è¶…æ—¶æ—¶é—´300ç§’
gather_timeout = 300    # è®¾ç½®è¶…æ—¶æ—¶é—´300ç§’
```

## 

## ğŸ ä½¿ç”¨äºŒè¿›åˆ¶éƒ¨ç½²ï¼ˆæ¨èï¼‰

### å¿…è¯»è¯´æ˜

**ï¼ˆ1ï¼‰èŠ‚ç‚¹è§„åˆ’**

:::tip

**NodeèŠ‚ç‚¹**ï¼šMaster æˆ– Worker æˆ– Master+Worker èŠ‚ç‚¹æˆ‘ä»¬ç»Ÿç§°ä¸ºNodeèŠ‚ç‚¹

**MasterèŠ‚ç‚¹**ï¼šä»…éƒ¨ç½²apiserverã€kube-controller-managerå’Œkube-schedulerçš„èŠ‚ç‚¹

**WorkerèŠ‚ç‚¹**ï¼šä»…éƒ¨ç½²kubeletã€kube-proxyçš„èŠ‚ç‚¹

:::

| ä¸»æœºå | NodeèŠ‚ç‚¹ | MasterèŠ‚ç‚¹ | WorkerèŠ‚ç‚¹ | EtcdèŠ‚ç‚¹ | å†…å­˜ | CPU  | é™æ€IP         |
| ------ | -------- | ---------- | ---------- | -------- | ---- | ---- | -------------- |
| node-1 | âœ”        | âœ”          |            | âœ”        | 2G   | 2æ ¸  | 192.168.48.142 |
| node-2 | âœ”        | âœ”          | âœ”          | âœ”        | 2G   | 2æ ¸  | 192.168.48.143 |
| node-3 | âœ”        |            | âœ”          | âœ”        | 2G   | 2æ ¸  | 192.168.48.144 |

**ï¼ˆ2ï¼‰ç‰ˆæœ¬è¯´æ˜**

| ç»„ä»¶       | ç‰ˆæœ¬         | å¤‡æ³¨                 |
| ---------- | ------------ | -------------------- |
| OS         | `Centos 7.9` |                      |
| kubernetes | `v1.24.4`    | åŒæ ·æ”¯æŒéƒ¨ç½²å…¶ä»–ç‰ˆæœ¬ |

**ï¼ˆ3ï¼‰ç§‘å­¦ä¸Šç½‘**

åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­éœ€è¦å»æµ·å¤–ä¸‹è½½éƒ¨åˆ†é•œåƒï¼Œéœ€è¦ä¸»æœºèƒ½å¤Ÿç§‘å­¦ä¸Šç½‘ï¼Œæˆ–è€…æå‰ä¸‹è½½åˆ°æœ¬åœ°å†ä¸Šä¼ åˆ°æœåŠ¡å™¨ä¸­

**ï¼ˆ4ï¼‰æœ€ä½é…ç½®**

æ”¯æŒä¸»æµç³»ç»Ÿï¼Œå†…å­˜æœ€ä½2Gï¼ŒCPUæœ€ä½2æ ¸ï¼Œç£ç›˜30Gä»¥ä¸Š

### ä¸­è½¬èŠ‚ç‚¹

ä¸ºäº†æ–¹ä¾¿æ–‡ä»¶çš„åˆ†å‘ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªä¸­è½¬èŠ‚ç‚¹ï¼ˆéšä¾¿ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå¯ä»¥æ˜¯é›†ç¾¤ä¸­çš„ä¹Ÿå¯ä»¥æ˜¯éé›†ç¾¤ä¸­çš„ï¼‰ï¼Œé…ç½®å¥½è·Ÿå…¶ä»–æ‰€æœ‰èŠ‚ç‚¹çš„å…å¯†ç™»å½•

```bash
# ç”Ÿæˆå¯†é’¥å¯¹
[root@node-1 ~]# ssh-keygen -t rsa

# é…ç½®å…å¯†ç™»å½•
[root@node-1 ~]# ssh-copy-id root@node-1
[root@node-1 ~]# ssh-copy-id root@node-2
[root@node-1 ~]# ssh-copy-id root@node-3
```

### ä¸‹è½½è½¯ä»¶åŒ…

::: tip kubernetesä¸‹è½½åœ°å€æ˜¯å¦‚ä½•æ¥çš„?

1. æ‰“å¼€Github Kubernetes Releasesé¡µé¢ï¼š[https://github.com/kubernetes/kubernetes/releases/](https://github.com/kubernetes/kubernetes/releases/)

2. é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬åï¼Œç‚¹å‡»`See the CHANGELOG for more details`ä¸­çš„é“¾æ¥

3. æ ¹æ® `Client Binaries` å’Œ `Server Binaries`ä¸‹è½½äºŒè¿›åˆ¶åŒ…

   `Server Binaries`äºŒè¿›åˆ¶åŒ…ä¸­åŒ…å«äº†`Client Binaries`ä¸­çš„å¯æ‰§è¡Œå‘½ä»¤ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦ä¸‹è½½`Server Binaries`åŒ…å³å¯

:::

```bash
# ä¸­è½¬èŠ‚ç‚¹åˆ›å»ºè½¯ä»¶åŒ…ç›®å½•pkg
[root@node-1 ~]# mkdir -p pkg && cd pkg

# ä¸‹è½½K8SäºŒè¿›åˆ¶åŒ…
[root@node-1 pkg]# wget -c https://storage.googleapis.com/kubernetes-release/release/v1.24.4/kubernetes-server-linux-amd64.tar.gz
[root@node-1 pkg]# tar zxf kubernetes-server-linux-amd64.tar.gz
[root@node-1 pkg]# cd kubernetes && mkdir -p src && tar zxf  kubernetes-src.tar.gz -C ./src
[root@node-1 kubernetes]# cd ~/pkg/

# ä¸‹è½½Etcdè½¯ä»¶åŒ…
[root@node-1 pkg]# wget -c https://github.com/etcd-io/etcd/releases/download/v3.4.20/etcd-v3.4.20-linux-amd64.tar.gz
[root@node-1 pkg]# tar zxf etcd-v3.4.20-linux-amd64.tar.gz

# å¤‡æ³¨: ä¹Ÿå¯ä»¥å•ç‹¬ä¸‹è½½æŸä¸ªäºŒè¿›åˆ¶åŒ…
# wget https://storage.googleapis.com/kubernetes-release/release/v1.24.3/bin/linux/amd64/kubectl
```

### åˆ†å‘è½¯ä»¶åŒ…

```bash
# è¿›å…¥kubernetesç›®å½•
[root@node-1 ~]# cd ~/pkg/kubernetes/server/bin/

# MasterèŠ‚ç‚¹
[root@node-1 bin]# MASTERS=(node-1 node-2) ; for instance in ${MASTERS[@]}; do
  scp kube-apiserver \
      kube-controller-manager \
      kube-scheduler \
      kubectl \
  root@${instance}:/usr/local/bin/
done

# NodeèŠ‚ç‚¹
[root@node-1 bin]# NODES=(node-1 node-2 node-3) ; for instance in ${NODES[@]}; do
  scp kubelet \
      kube-proxy \
  root@${instance}:/usr/local/bin/
done

# --------------------------------------------------------------------------------------------------------
# è¿›å…¥etcdç›®å½•
[root@node-1 bin]# cd ~/pkg/etcd-v3.4.20-linux-amd64/

# EtcdèŠ‚ç‚¹
[root@node-1 etcd-v3.4.20-linux-amd64]# ETCDS=(node-1 node-2 node-3) ; for instance in ${ETCDS[@]}; do
  scp etcd \
      etcdctl \
  root@${instance}:/usr/local/bin/
done
```

### ç”ŸæˆSSLè¯ä¹¦

#### **å‡†å¤‡å·¥ä½œï¼šä¸‹è½½cfsslå·¥å…·**

```bash
# ä¸‹è½½äºŒè¿›åˆ¶å·¥å…·
[root@node-1 ~]# wget https://github.com/cloudflare/cfssl/releases/download/v1.6.1/cfssl_1.6.1_linux_amd64 -O /usr/local/bin/cfssl && \
                 wget https://github.com/cloudflare/cfssl/releases/download/v1.6.1/cfssljson_1.6.1_linux_amd64 -O /usr/local/bin/cfssljson && \
                 chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson

# æŸ¥çœ‹ç‰ˆæœ¬
[root@node-1 ~]# cfssl version && echo && cfssljson --version
Version: 1.6.1
Runtime: go1.12.12

Version: 1.6.1
Runtime: go1.12.12
```

#### ä¸­è½¬èŠ‚ç‚¹è¯ä¹¦ç›®å½•pki

```bash
# åœ¨ä¸­è½¬èŠ‚ç‚¹åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„è¯ä¹¦ç›®å½•
[root@node-1 ~]# mkdir -p ~/pki && cd ~/pki
```

#### **ï¼ˆ1ï¼‰CAè¯ä¹¦**

æ ¹è¯ä¹¦ï¼ˆCA è¯ä¹¦ï¼‰æ˜¯é›†ç¾¤æ‰€æœ‰èŠ‚ç‚¹å…±äº«çš„ï¼Œåªéœ€è¦åˆ›å»ºä¸€ä¸ªæ ¹è¯ä¹¦ï¼ˆCA è¯ä¹¦ï¼‰ï¼Œåç»­åˆ›å»ºçš„æ‰€æœ‰è¯ä¹¦éƒ½ç”±å®ƒç­¾å

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
# åˆ›å»ºæ ¹è¯ä¹¦é…ç½®æ–‡ä»¶ï¼ˆè¿‡æœŸæ—¶é—´ 876000h/24/365 = 100å¹´ï¼‰
[root@node-1 pki]# cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "876000h"
    },
    "profiles": {
      "kubernetes": {
        "expiry": "876000h",
        "usages": [
          "signing",
          "key encipherment",
          "server auth",
          "client auth"
        ]
      }
    }
  }
}
EOF

# åˆ›å»ºæ ¹è¯ä¹¦ç­¾åè¯·æ±‚æ–‡ä»¶
[root@node-1 pki]# cat > ca-csr.json <<EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Oregon"
    }
  ]
}
EOF

# ç”Ÿæˆæ ¹è¯ä¹¦å’Œç§é’¥
[root@node-1 pki]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca

2022/08/21 11:46:56 [INFO] generating a new CA key and certificate from CSR
2022/08/21 11:46:56 [INFO] generate received request
2022/08/21 11:46:56 [INFO] received CSR
2022/08/21 11:46:56 [INFO] generating key: rsa-2048
2022/08/21 11:46:56 [INFO] encoded CSR
2022/08/21 11:46:56 [INFO] signed certificate with serial number 253271716697038775687884636491121935350121536450

[root@node-1 pki]# ls -l
total 20
-rw-r--r-- 1 root root  236 Aug 16 03:00 ca-config.json
-rw-r--r-- 1 root root 1005 Aug 16 03:00 ca.csr
-rw-r--r-- 1 root root  211 Aug 16 03:00 ca-csr.json
-rw------- 1 root root 1679 Aug 16 03:00 ca-key.pem   # CAè¯ä¹¦ç§é’¥
-rw-r--r-- 1 root root 1318 Aug 16 03:00 ca.pem       # CAè¯ä¹¦
```

:::

#### ï¼ˆ2ï¼‰SAè¯ä¹¦

* Service Accountè¯ä¹¦ï¼Œé›†ç¾¤å…±äº«ä¸€ä»½è¯ä¹¦

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
[root@node-1 pki]# cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "seven"
    }
  ]
}
EOF

[root@node-1 pki]# cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  service-account-csr.json | cfssljson -bare service-account
  
2022/08/21 11:51:10 [INFO] generate received request
2022/08/21 11:51:10 [INFO] received CSR
2022/08/21 11:51:10 [INFO] generating key: rsa-2048
2022/08/21 11:51:11 [INFO] encoded CSR
2022/08/21 11:51:11 [INFO] signed certificate with serial number 175935166455417068459398438005389278116292115335
2022/08/21 11:51:11 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").

[root@node-1 pki]# ls -l | grep service-account
-rw-r--r-- 1 root root 1009 Aug 21 11:51 service-account.csr
-rw-r--r-- 1 root root  213 Aug 21 11:51 service-account-csr.json
-rw------- 1 root root 1679 Aug 21 11:51 service-account-key.pem
-rw-r--r-- 1 root root 1407 Aug 21 11:51 service-account.pem
```

:::

#### **ï¼ˆ3ï¼‰adminè¯ä¹¦**

adminç”¨æˆ·è¯ä¹¦ï¼Œé›†ç¾¤å†…åªéœ€è¦åˆ›å»ºä¸€ä»½å³å¯

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
# adminå®¢æˆ·ç«¯è¯ä¹¦é…ç½®æ–‡ä»¶
[root@node-1 pki]# cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "system:masters",
      "OU": "seven"
    }
  ]
}
EOF

# ç”Ÿæˆè¯ä¹¦
[root@node-1 pki]# cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin

2022/08/21 11:47:28 [INFO] generate received request
2022/08/21 11:47:28 [INFO] received CSR
2022/08/21 11:47:28 [INFO] generating key: rsa-2048
2022/08/21 11:47:28 [INFO] encoded CSR
2022/08/21 11:47:28 [INFO] signed certificate with serial number 651243122780313242439194972461572344701530496872
2022/08/21 11:47:28 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").

[root@node-1 pki]# ls -l | grep admin
-rw-r--r-- 1 root root 1009 Aug 21 11:47 admin.csr
-rw-r--r-- 1 root root  213 Aug 21 11:47 admin-csr.json
-rw------- 1 root root 1675 Aug 21 11:47 admin-key.pem
-rw-r--r-- 1 root root 1407 Aug 21 11:47 admin.pem
```

:::

#### ï¼ˆ4ï¼‰kubeletè¯ä¹¦

* Kubernetesä½¿ç”¨ä¸€ç§ç§°ä¸ºNode Authorizerçš„ä¸“ç”¨æˆæƒæ¨¡å¼æ¥æˆæƒKubeletså‘å‡ºçš„APIè¯·æ±‚ã€‚ 

  Kubeletä½¿ç”¨å°†å…¶æ ‡è¯†ä¸º`system:nodes`ç»„ä¸­çš„å‡­æ®ï¼Œå…¶ç”¨æˆ·åä¸º`system:node:<nodeName>`

* æ¯ä¸ªNodeä½¿ç”¨è‡ªå·±çš„è¯ä¹¦

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
# è®¾ç½®Nodeåˆ—è¡¨
[root@node-1 pki]# NODES=(node-1 node-2 node-3)
[root@node-1 pki]# NODE_IPS=(192.168.48.142 192.168.48.143 192.168.48.144)

# ç”Ÿæˆæ‰€æœ‰NodeèŠ‚ç‚¹çš„è¯ä¹¦é…ç½®
[root@node-1 pki]# for ((i=0;i<${#NODES[@]};i++)); do
cat > ${NODES[$i]}-csr.json <<EOF
{
  "CN": "system:node:${NODES[$i]}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Beijing",
      "O": "system:nodes",
      "OU": "seven",
      "ST": "Beijing"
    }
  ]
}
EOF
cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${NODES[$i]},${NODE_IPS[$i]} \
  -profile=kubernetes \
  ${NODES[$i]}-csr.json | cfssljson -bare ${NODES[$i]}
done

2022/08/21 11:47:59 [INFO] generate received request
2022/08/21 11:47:59 [INFO] received CSR
2022/08/21 11:47:59 [INFO] generating key: rsa-2048
2022/08/21 11:47:59 [INFO] encoded CSR
2022/08/21 11:47:59 [INFO] signed certificate with serial number 99925140828288407693517684936448811202438995925
2022/08/21 11:47:59 [INFO] generate received request
2022/08/21 11:47:59 [INFO] received CSR
2022/08/21 11:47:59 [INFO] generating key: rsa-2048
2022/08/21 11:48:00 [INFO] encoded CSR
2022/08/21 11:48:00 [INFO] signed certificate with serial number 638244081209209689287400959807600586650469594148
2022/08/21 11:48:00 [INFO] generate received request
2022/08/21 11:48:00 [INFO] received CSR
2022/08/21 11:48:00 [INFO] generating key: rsa-2048
2022/08/21 11:48:00 [INFO] encoded CSR
2022/08/21 11:48:00 [INFO] signed certificate with serial number 382832994393608753859327324523279605896687730207

[root@node-1 pki]# ls -l | grep node
-rw-r--r-- 1 root root 1078 Aug 21 11:47 node-1.csr
-rw-r--r-- 1 root root  224 Aug 21 11:47 node-1-csr.json
-rw------- 1 root root 1679 Aug 21 11:47 node-1-key.pem
-rw-r--r-- 1 root root 1456 Aug 21 11:47 node-1.pem
-rw-r--r-- 1 root root 1078 Aug 21 11:48 node-2.csr
-rw-r--r-- 1 root root  224 Aug 21 11:47 node-2-csr.json
-rw------- 1 root root 1679 Aug 21 11:48 node-2-key.pem
-rw-r--r-- 1 root root 1456 Aug 21 11:48 node-2.pem
-rw-r--r-- 1 root root 1078 Aug 21 11:48 node-3.csr
-rw-r--r-- 1 root root  224 Aug 21 11:48 node-3-csr.json
-rw------- 1 root root 1675 Aug 21 11:48 node-3-key.pem
-rw-r--r-- 1 root root 1456 Aug 21 11:48 node-3.pem
```

:::

#### ï¼ˆ5ï¼‰kube-proxyè¯ä¹¦

* æ‰€æœ‰NodeèŠ‚ç‚¹å…±äº«ä¸€ä»½è¯ä¹¦

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
[root@node-1 pki]# cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "seven"
    }
  ]
}
EOF

[root@node-1 pki]# cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy
  
2022/08/21 11:49:01 [INFO] generate received request
2022/08/21 11:49:01 [INFO] received CSR
2022/08/21 11:49:01 [INFO] generating key: rsa-2048
2022/08/21 11:49:01 [INFO] encoded CSR
2022/08/21 11:49:01 [INFO] signed certificate with serial number 109829974994790485239297069683729116680084539008
2022/08/21 11:49:01 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").

[root@node-1 pki]# ls -l | grep kube-proxy
-rw-r--r-- 1 root root 1009 Aug 21 11:49 kube-proxy.csr
-rw-r--r-- 1 root root  214 Aug 21 11:48 kube-proxy-csr.json
-rw------- 1 root root 1679 Aug 21 11:49 kube-proxy-key.pem
-rw-r--r-- 1 root root 1407 Aug 21 11:49 kube-proxy.pem
```

:::

#### ï¼ˆ6ï¼‰proxy-client è¯ä¹¦

* æ‰€æœ‰NodeèŠ‚ç‚¹å…±äº«ä¸€ä»½è¯ä¹¦

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
[root@node-1 pki]# cat > proxy-client-csr.json <<EOF
{
  "CN": "aggregator",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "seven"
    }
  ]
}
EOF

[root@node-1 pki]# cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  proxy-client-csr.json | cfssljson -bare proxy-client

2022/08/21 11:51:35 [INFO] generate received request
2022/08/21 11:51:35 [INFO] received CSR
2022/08/21 11:51:35 [INFO] generating key: rsa-2048
2022/08/21 11:51:36 [INFO] encoded CSR
2022/08/21 11:51:36 [INFO] signed certificate with serial number 624599378216669470989725514687740733197475898533
2022/08/21 11:51:36 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").

[root@node-1 pki]# ls -l | grep proxy-client
-rw-r--r-- 1 root root 1001 Aug 21 11:51 proxy-client.csr
-rw-r--r-- 1 root root  207 Aug 21 11:51 proxy-client-csr.json
-rw------- 1 root root 1675 Aug 21 11:51 proxy-client-key.pem
-rw-r--r-- 1 root root 1399 Aug 21 11:51 proxy-client.pem
```

:::

#### ï¼ˆ7ï¼‰kube-apiserverè¯ä¹¦

* æœåŠ¡ç«¯è¯ä¹¦ä¸å®¢æˆ·ç«¯ç•¥æœ‰ä¸åŒï¼Œå®¢æˆ·ç«¯éœ€è¦é€šè¿‡ä¸€ä¸ªåå­—æˆ–è€…ä¸€ä¸ªipå»è®¿é—®æœåŠ¡ç«¯ï¼Œ

  æ‰€ä»¥è¯ä¹¦å¿…é¡»è¦åŒ…å«å®¢æˆ·ç«¯æ‰€è®¿é—®çš„åå­—æˆ–ipï¼Œç”¨ä»¥å®¢æˆ·ç«¯éªŒè¯

* æ‰€æœ‰MasterèŠ‚ç‚¹å…±äº«ä¸€ä»½è¯ä¹¦

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
[root@node-1 pki]# cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "seven"
    }
  ]
}
EOF

# apiserverçš„service ipåœ°å€ï¼ˆä¸€èˆ¬æ˜¯svcç½‘æ®µçš„ç¬¬ä¸€ä¸ªipï¼‰
[root@node-1 pki]# KUBERNETES_SVC_IP=10.233.0.1

# æ‰€æœ‰çš„masterå†…ç½‘ipï¼Œé€—å·åˆ†éš”ï¼ˆäº‘ç¯å¢ƒå¯ä»¥åŠ ä¸Šmasterå…¬ç½‘ipä»¥ä¾¿æ”¯æŒå…¬ç½‘ipè®¿é—®ï¼‰
[root@node-1 pki]# MASTER_IPS=192.168.48.142,192.168.48.143,192.168.48.144

# ç”Ÿæˆè¯ä¹¦
[root@node-1 pki]# cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${KUBERNETES_SVC_IP},${MASTER_IPS},127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes
  
2022/08/21 11:50:44 [INFO] generate received request
2022/08/21 11:50:44 [INFO] received CSR
2022/08/21 11:50:44 [INFO] generating key: rsa-2048
2022/08/21 11:50:44 [INFO] encoded CSR
2022/08/21 11:50:44 [INFO] signed certificate with serial number 376785325346225517588814760780559674014863525105

[root@node-1 pki]# ls -l | grep kubernetes
-rw-r--r-- 1 root root 1249 Aug 21 11:50 kubernetes.csr
-rw-r--r-- 1 root root  207 Aug 21 11:49 kubernetes-csr.json
-rw------- 1 root root 1675 Aug 21 11:50 kubernetes-key.pem
-rw-r--r-- 1 root root 1623 Aug 21 11:50 kubernetes.pem
```

:::

#### ï¼ˆ8ï¼‰kube-schedulerè¯ä¹¦

* æ‰€æœ‰MasterèŠ‚ç‚¹å…±äº«ä¸€ä»½è¯ä¹¦

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
[root@node-1 pki]# cat > kube-scheduler-csr.json <<EOF
{
    "CN": "system:kube-scheduler",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "system:kube-scheduler",
        "OU": "seven"
      }
    ]
}
EOF

[root@node-1 pki]# cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-scheduler-csr.json | cfssljson -bare kube-scheduler
  
2022/08/21 11:49:26 [INFO] generate received request
2022/08/21 11:49:26 [INFO] received CSR
2022/08/21 11:49:26 [INFO] generating key: rsa-2048
2022/08/21 11:49:27 [INFO] encoded CSR
2022/08/21 11:49:27 [INFO] signed certificate with serial number 529607043360813455375991599802579655701759291101
2022/08/21 11:49:27 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").

[root@node-1 pki]# ls -l | grep kube-scheduler
-rw-r--r-- 1 root root 1041 Aug 21 11:49 kube-scheduler.csr
-rw-r--r-- 1 root root  268 Aug 21 11:49 kube-scheduler-csr.json
-rw------- 1 root root 1679 Aug 21 11:49 kube-scheduler-key.pem
-rw-r--r-- 1 root root 1440 Aug 21 11:49 kube-scheduler.pem
```

:::

#### ï¼ˆ9ï¼‰kube-controller-managerè¯ä¹¦

* æ‰€æœ‰MasterèŠ‚ç‚¹å…±äº«ä¸€ä»½è¯ä¹¦

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
[root@node-1 pki]# cat > kube-controller-manager-csr.json <<EOF
{
    "CN": "system:kube-controller-manager",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "system:kube-controller-manager",
        "OU": "seven"
      }
    ]
}
EOF

[root@node-1 pki]# cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
  
2022/08/21 11:48:32 [INFO] generate received request
2022/08/21 11:48:32 [INFO] received CSR
2022/08/21 11:48:32 [INFO] generating key: rsa-2048
2022/08/21 11:48:32 [INFO] encoded CSR
2022/08/21 11:48:32 [INFO] signed certificate with serial number 611097625429267385505563611927508226697109199115
2022/08/21 11:48:32 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").

[root@node-1 pki]# ls -l | grep kube-controller-manager
-rw-r--r-- 1 root root 1066 Aug 21 11:48 kube-controller-manager.csr
-rw-r--r-- 1 root root  286 Aug 21 11:48 kube-controller-manager-csr.json
-rw------- 1 root root 1679 Aug 21 11:48 kube-controller-manager-key.pem
-rw-r--r-- 1 root root 1464 Aug 21 11:48 kube-controller-manager.pem
```

:::

#### åˆ†å‘è¯ä¹¦ï¼šNodeã€Masterã€Etcd

ï¼ˆ1ï¼‰åˆ†å‘NodeèŠ‚ç‚¹éœ€è¦çš„è¯ä¹¦å’Œç§é’¥

```bash
[root@node-1 pki]# NODES=(node-1 node-2 node-3) ; for instance in ${NODES[@]}; do
    rsync -avzp \
        ca.pem \
        ${instance}-key.pem \
        ${instance}.pem \
    root@${instance}:~/tmp.node.ssl/
done
```

ï¼ˆ2ï¼‰åˆ†å‘MasterèŠ‚ç‚¹éœ€è¦çš„è¯ä¹¦å’Œç§é’¥

```bash
[root@node-1 pki]# MASTERS=(node-1 node-2) ; for instance in ${MASTERS[@]}; do
    rsync -avzp \
        ca.pem \
        ca-key.pem \
        kubernetes-key.pem \
        kubernetes.pem \
        service-account-key.pem \
        service-account.pem \
        proxy-client.pem \
        proxy-client-key.pem \
    root@${instance}:~/tmp.master.ssl/
done
```

ï¼ˆ3ï¼‰åˆ†å‘EtcdèŠ‚ç‚¹éœ€è¦çš„è¯ä¹¦å’Œç§é’¥

```bash
[root@node-1 pki]# ETCDS=(node-1 node-2 node-3) ; for instance in ${ETCDS[@]}; do	
	rsync -avzp \
        ca.pem \
        kubernetes-key.pem \
        kubernetes.pem \
    root@${instance}:~/tmp.etcd.ssl/
done
```

### è®¤è¯é…ç½®

::: tip

kubernetesçš„è®¤è¯é…ç½®æ–‡ä»¶ï¼Œä¹Ÿå«kubeconfigsï¼Œç”¨äºè®©kubernetesçš„å®¢æˆ·ç«¯å®šä½kube-apiserverå¹¶é€šè¿‡apiserverçš„å®‰å…¨è®¤è¯ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬ä¸€èµ·æ¥ç”Ÿæˆå„ä¸ªç»„ä»¶çš„kubeconfigsï¼ŒåŒ…æ‹¬controller-managerï¼Œkubeletï¼Œkube-proxyï¼Œschedulerï¼Œä»¥åŠadminç”¨æˆ·

:::

#### ä¸­è½¬èŠ‚ç‚¹é…ç½®æ–‡ä»¶ç›®å½•kubeconfig

```bash
# åœ¨ä¸­è½¬èŠ‚ç‚¹åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„é…ç½®æ–‡ä»¶ç›®å½•
[root@node-1 ~]# mkdir ~/kubeconfig && cd ~/kubeconfig
```

#### ï¼ˆ1ï¼‰kubelet

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
# æŒ‡å®šä½ çš„workeråˆ—è¡¨ï¼ˆhostnameï¼‰ï¼Œç©ºæ ¼åˆ†éš”
[root@node-1 kubeconfig]# NODES="node-1 node-2 node-3" ; for instance in ${NODES}; do
  kubectl config set-cluster kubernetes \
    --certificate-authority=/root/pki/ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-credentials system:node:${instance} \
    --client-certificate=/root/pki/${instance}.pem \
    --client-key=/root/pki/${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:node:${instance} \
    --kubeconfig=${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=${instance}.kubeconfig
done

Cluster "kubernetes" set.
User "system:node:node-1" set.
Context "default" created.
Switched to context "default".
Cluster "kubernetes" set.
User "system:node:node-2" set.
Context "default" created.
Switched to context "default".
Cluster "kubernetes" set.
User "system:node:node-3" set.
Context "default" created.
Switched to context "default".

[root@node-1 kubeconfig]# ls -l
total 24
-rw------- 1 root root 6305 Aug 22 19:37 node-1.kubeconfig
-rw------- 1 root root 6301 Aug 22 19:37 node-2.kubeconfig
-rw------- 1 root root 6301 Aug 22 19:37 node-3.kubeconfig
```

:::

#### ï¼ˆ2ï¼‰kube-proxy

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
kubectl config set-cluster kubernetes \
    --certificate-authority=/root/pki/ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials system:kube-proxy \
   --client-certificate=/root/pki/kube-proxy.pem \
   --client-key=/root/pki/kube-proxy-key.pem \
   --embed-certs=true \
   --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
   --cluster=kubernetes \
   --user=system:kube-proxy \
   --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

[root@node-1 kubeconfig]# ls -l | grep kube-proxy.kubeconfig
-rw------- 1 root root 6.1K Aug 22 20:22 kube-proxy.kubeconfig
```

:::

#### ï¼ˆ3ï¼‰kube-controller-manager

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
kubectl config set-cluster kubernetes \
  --certificate-authority=/root/pki/ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=/root/pki/kube-controller-manager.pem \
  --client-key=/root/pki/kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig

[root@node-1 kubeconfig]# ls -l | grep kube-controller-manager.kubeconfig
-rw------- 1 root root 6333 Aug 22 20:23 kube-controller-manager.kubeconfig
```

:::

#### ï¼ˆ4ï¼‰kube-scheduler

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
kubectl config set-cluster kubernetes \
  --certificate-authority=/root/pki/ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=/root/pki/kube-scheduler.pem \
  --client-key=/root/pki/kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig

[root@node-1 kubeconfig]# ls -l | grep kube-scheduler.kubeconfig
-rw------- 1 root root 6283 Aug 22 20:24 kube-scheduler.kubeconfig
```

:::

#### ï¼ˆ5ï¼‰admin

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
kubectl config set-cluster kubernetes \
  --certificate-authority=/root/pki/ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=admin.kubeconfig

kubectl config set-credentials admin \
  --client-certificate=/root/pki/admin.pem \
  --client-key=/root/pki/admin-key.pem \
  --embed-certs=true \
  --kubeconfig=admin.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=admin \
  --kubeconfig=admin.kubeconfig

kubectl config use-context default --kubeconfig=admin.kubeconfig

[root@node-1 kubeconfig]# ls -l | grep admin.kubeconfig
-rw------- 1 root root 6207 Aug 22 20:25 admin.kubeconfig
```

:::

#### åˆ†å‘é…ç½®æ–‡ä»¶ï¼šNodeã€Master

æŠŠkubeletå’Œkube-proxyéœ€è¦çš„kubeconfigé…ç½®åˆ†å‘åˆ°æ¯ä¸ªNodeèŠ‚ç‚¹

```bash
[root@node-1 kubeconfig]# NODES="node-1 node-2 node-3" ; for instance in ${NODES}; do
    rsync -avzp \
        ${instance}.kubeconfig \
    	kube-proxy.kubeconfig \
	${instance}:~/tmp.node.kubeconfig/
done
```

æŠŠkube-controller-managerå’Œkube-scheduleréœ€è¦çš„kubeconfigé…ç½®åˆ†å‘åˆ°MasterèŠ‚ç‚¹

```bash
[root@node-1 kubeconfig]# MASTERS="node-1 node-2" ; for instance in ${MASTERS}; do
    rsync -avzp \
        admin.kubeconfig \
        kube-controller-manager.kubeconfig \
        kube-scheduler.kubeconfig \
    ${instance}:~/tmp.master.kubeconfig/
done
```

### éƒ¨ç½²Etcdé›†ç¾¤

:::tip

ä»¥ä¸‹æ“ä½œåœ¨æ‰€æœ‰EtcdèŠ‚ç‚¹æ‰§è¡Œ

:::

ï¼ˆ1ï¼‰æ‹·è´etcdè¯ä¹¦

```bash
[root@node-1 ~]# mkdir -p /etc/etcd/ssl /var/lib/etcd && chmod 700 /var/lib/etcd
[root@node-1 ~]# mv ~/tmp.etcd.ssl/ca.pem \
                    ~/tmp.etcd.ssl/kubernetes.pem \
                    ~/tmp.etcd.ssl/kubernetes-key.pem \
                 /etc/etcd/ssl/
[root@node-1 ~]# rmdir ~/tmp.etcd.ssl
```

ï¼ˆ2ï¼‰é…ç½®etcd.serviceæ–‡ä»¶

::: details ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å‘½ä»¤

```bash
ETCD_NAME=$(hostname -s)
ETCD_IP=192.168.48.144

# etcdæ‰€æœ‰èŠ‚ç‚¹çš„ipåœ°å€
ETCD_NAMES=(node-1 node-2 node-3)
ETCD_IPS=(192.168.48.142 192.168.48.143 192.168.48.144)

cat >/etc/systemd/system/etcd.service <<EOF
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/ssl/kubernetes.pem \\
  --key-file=/etc/etcd/ssl/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/ssl/kubernetes.pem \\
  --peer-key-file=/etc/etcd/ssl/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ssl/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${ETCD_IP}:2380 \\
  --listen-peer-urls https://${ETCD_IP}:2380 \\
  --listen-client-urls https://${ETCD_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${ETCD_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster ${ETCD_NAMES[0]}=https://${ETCD_IPS[0]}:2380,${ETCD_NAMES[1]}=https://${ETCD_IPS[1]}:2380,${ETCD_NAMES[2]}=https://${ETCD_IPS[2]}:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

:::

ï¼ˆ3ï¼‰å¯åŠ¨etcdé›†ç¾¤

```bash
systemctl daemon-reload && systemctl enable etcd && systemctl restart etcd
```

ï¼ˆ4ï¼‰éªŒè¯etcdé›†ç¾¤çŠ¶æ€

```bash
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ssl/ca.pem \
  --cert=/etc/etcd/ssl/kubernetes.pem \
  --key=/etc/etcd/ssl/kubernetes-key.pem
  
2c3beb1e7481123e, started, node-2, https://192.168.48.143:2380, https://192.168.48.143:2379, false
369510e61aee9b6f, started, node-3, https://192.168.48.144:2380, https://192.168.48.144:2379, false
e8775739ad328e98, started, node-1, https://192.168.48.142:2380, https://192.168.48.142:2379, false  
```

### éƒ¨ç½²Kubernetes MasterèŠ‚ç‚¹

:::tip

ä»¥ä¸‹æ“ä½œåœ¨æ‰€æœ‰MasterèŠ‚ç‚¹æ‰§è¡Œ

:::

#### éƒ¨ç½²apiserver

```bash
# å‡†å¤‡è¯ä¹¦æ–‡ä»¶
mkdir -p /etc/kubernetes/ssl

mv ~/tmp.master.ssl/ca.pem \
   ~/tmp.master.ssl/ca-key.pem \
   ~/tmp.master.ssl/kubernetes.pem \
   ~/tmp.master.ssl/kubernetes-key.pem \
   ~/tmp.master.ssl/service-account.pem \
   ~/tmp.master.ssl/service-account-key.pem \
   ~/tmp.master.ssl/proxy-client.pem \
   ~/tmp.master.ssl/proxy-client-key.pem \
/etc/kubernetes/ssl

rmdir ~/tmp.master.ssl

# é…ç½®kube-apiserver.service
# æœ¬æœºå†…ç½‘ip
IP=192.168.48.143
# apiserverå®ä¾‹æ•°
APISERVER_COUNT=2
# etcdèŠ‚ç‚¹
ETCD_ENDPOINTS=(192.168.48.142 192.168.48.143 192.168.48.144)
# åˆ›å»º apiserver service
cat >/etc/systemd/system/kube-apiserver.service <<EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${IP} \\
  --allow-privileged=true \\
  --apiserver-count=${APISERVER_COUNT} \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\
  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\
  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --etcd-servers=https://${ETCD_ENDPOINTS[0]}:2379,https://${ETCD_ENDPOINTS[1]}:2379,https://${ETCD_ENDPOINTS[2]}:2379 \\
  --event-ttl=1h \\
  --kubelet-certificate-authority=/etc/kubernetes/ssl/ca.pem \\
  --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \\
  --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --service-account-issuer=api \\
  --service-account-key-file=/etc/kubernetes/ssl/service-account.pem \\
  --service-account-signing-key-file=/etc/kubernetes/ssl/service-account-key.pem \\
  --api-audiences=api,vault,factors \\
  --service-cluster-ip-range=10.233.0.0/16 \\
  --service-node-port-range=30000-32767 \\
  --proxy-client-cert-file=/etc/kubernetes/ssl/proxy-client.pem \\
  --proxy-client-key-file=/etc/kubernetes/ssl/proxy-client-key.pem \\
  --runtime-config=api/all=true \\
  --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --requestheader-allowed-names=aggregator \\
  --requestheader-extra-headers-prefix=X-Remote-Extra- \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\
  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --v=1
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

#### éƒ¨ç½²kube-controller-manager

```bash
# å‡†å¤‡kubeconfigé…ç½®æ–‡ä»¶
mv ~/tmp.master.kubeconfig/kube-controller-manager.kubeconfig /etc/kubernetes/

# åˆ›å»º kube-controller-manager.service
cat >/etc/systemd/system/kube-controller-manager.service <<EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --bind-address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\
  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\
  --cluster-signing-duration=876000h0m0s \\
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --service-account-private-key-file=/etc/kubernetes/ssl/service-account-key.pem \\
  --service-cluster-ip-range=10.233.0.0/16 \\
  --use-service-account-credentials=true \\
  --v=1
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

#### éƒ¨ç½²kube-scheduler

```bash
# å‡†å¤‡kubeconfigé…ç½®æ–‡ä»¶
mv ~/tmp.master.kubeconfig/kube-scheduler.kubeconfig /etc/kubernetes

# åˆ›å»º scheduler service æ–‡ä»¶
cat >/etc/systemd/system/kube-scheduler.service <<EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --leader-elect=true \\
  --bind-address=0.0.0.0 \\
  --v=1
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

#### å¯åŠ¨æœåŠ¡

```bash
systemctl daemon-reload

systemctl restart kube-apiserver
systemctl restart kube-controller-manager
systemctl restart kube-scheduler

systemctl enable kube-apiserver
systemctl enable kube-controller-manager
systemctl enable kube-scheduler

# æ£€æŸ¥æœåŠ¡
[root@node-1 ~]# netstat -tlnpu | grep kube
tcp6       0      0 :::10259                :::*                    LISTEN      8851/kube-scheduler 
tcp6       0      0 :::6443                 :::*                    LISTEN      8653/kube-apiserver 
tcp6       0      0 :::10257                :::*                    LISTEN      8660/kube-controlle
```

#### é…ç½®kubectl

kubectlæ˜¯ç”¨æ¥ç®¡ç†kubernetesé›†ç¾¤çš„å®¢æˆ·ç«¯å·¥å…·ï¼Œå‰é¢æˆ‘ä»¬å·²ç»ä¸‹è½½åˆ°äº†æ‰€æœ‰çš„masterèŠ‚ç‚¹ã€‚ä¸‹é¢æˆ‘ä»¬æ¥é…ç½®è¿™ä¸ªå·¥å…·ï¼Œè®©å®ƒå¯ä»¥ä½¿ç”¨ã€‚

```bash
# åˆ›å»ºkubectlçš„é…ç½®ç›®å½•
mkdir ~/.kube/

# æŠŠç®¡ç†å‘˜çš„é…ç½®æ–‡ä»¶ç§»åŠ¨åˆ°kubectlçš„é»˜è®¤ç›®å½•
mv ~/tmp.master.kubeconfig/admin.kubeconfig ~/.kube/config

# æµ‹è¯•
kubectl get nodes  # è¾“å‡ºç»“æœ No resources found

# åœ¨æ‰§è¡Œkubectl execã€runã€logsç­‰å‘½ä»¤æ—¶ï¼Œapiserverä¼šè½¬å‘åˆ°kubelet
# è¿™é‡Œå®šä¹‰RBACè§„åˆ™å…è®¸apiserverè°ƒç”¨kubelet API
# åªéœ€è¦åœ¨ä»»æ„ä¸€ä¸ªMasterèŠ‚ç‚¹æ‰§è¡Œä¸€æ¬¡
kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
```

#### æ¸…ç†ä¸´æ—¶ç›®å½•

```bash
rmdir ~/tmp.master.kubeconfig
```

### éƒ¨ç½²Kubernetes NodeèŠ‚ç‚¹

#### éƒ¨ç½²Containerd

```bash
# è®¾å®šcontainerdçš„ç‰ˆæœ¬å·
VERSION=1.4.3

# ä¸‹è½½å‹ç¼©åŒ…
wget -c https://github.com/containerd/containerd/releases/download/v${VERSION}/cri-containerd-cni-${VERSION}-linux-amd64.tar.gz

# è§£å‹ç¼©
mkdir -p containerd
tar zxf cri-containerd-cni-${VERSION}-linux-amd64.tar.gz -C ./containerd

# å¤åˆ¶éœ€è¦çš„æ–‡ä»¶
cd ./containerd && \
cp etc/crictl.yaml /etc/ && \
cp etc/systemd/system/containerd.service /etc/systemd/system/ && \
cp -r usr /

# é…ç½®æ–‡ä»¶
mkdir -p /etc/containerd # åˆ›å»ºé…ç½®æ–‡ä»¶ç›®å½•
containerd config default > /etc/containerd/config.toml  # é»˜è®¤é…ç½®ç”Ÿæˆé…ç½®æ–‡ä»¶
vi /etc/containerd/config.toml  # å®šåˆ¶åŒ–é…ç½®(å¯é€‰ï¼Œè¿™é‡Œä¸åšä»»ä½•ä¿®æ”¹)

# å¯åŠ¨æœåŠ¡
systemctl restart containerd
systemctl enable containerd

# æ£€æŸ¥çŠ¶æ€
systemctl status containerd
```

#### éƒ¨ç½²kubelet

æ–‡æ¡£ï¼š

* [https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kubelet/](https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kubelet/)
* [https://kubernetes.io/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/](https://kubernetes.io/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)

```bash
# å‡†å¤‡è¯ä¹¦æ–‡ä»¶
mkdir -p /etc/kubernetes/ssl/

mv ~/tmp.node.ssl/ca.pem \
   ~/tmp.node.ssl/ca-key.pem \
   ~/tmp.node.ssl/${HOSTNAME}-key.pem \
   ~/tmp.node.ssl/${HOSTNAME}.pem \
/etc/kubernetes/ssl/

rmdir ~/tmp.node.ssl

# å‡†å¤‡kubeconfigé…ç½®æ–‡ä»¶
mv ~/tmp.node.kubeconfig/${HOSTNAME}.kubeconfig /etc/kubernetes/kubeconfig

# å†™å…¥kubeleté…ç½®æ–‡ä»¶
IP=192.168.48.142
cat >/etc/kubernetes/kubelet-config.yaml <<EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/etc/kubernetes/ssl/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "169.254.25.10"
podCIDR: "10.200.0.0/16"
address: ${IP}
readOnlyPort: 0
staticPodPath: /etc/kubernetes/manifests
healthzPort: 10248
healthzBindAddress: 127.0.0.1
kubeletCgroups: /systemd/system.slice
resolvConf: "/etc/resolv.conf"
runtimeRequestTimeout: "15m"
kubeReserved:
  cpu: 200m
  memory: 512M
tlsCertFile: "/etc/kubernetes/ssl/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/etc/kubernetes/ssl/${HOSTNAME}-key.pem"
registerNode: true
EOF

# å†™å…¥Systemd Serviceæ–‡ä»¶
cat >/etc/systemd/system/kubelet.service <<EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/etc/kubernetes/kubelet-config.yaml \\
  --kubeconfig=/etc/kubernetes/kubeconfig \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --node-ip=${IP} \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

#### éƒ¨ç½²nginx-proxy

* `nginx-proxy`æ˜¯ä¸€ä¸ªç”¨äºworkerèŠ‚ç‚¹è®¿é—®apiserverçš„ä¸€ä¸ªä»£ç†ï¼Œæ˜¯apiserverä¸€ä¸ªä¼˜é›…çš„é«˜å¯ç”¨æ–¹æ¡ˆ

  å®ƒä½¿ç”¨kubeletçš„staticpodæ–¹å¼å¯åŠ¨ï¼Œè®©æ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥å‡è¡¡çš„è®¿é—®åˆ°æ¯ä¸ªapiserveræœåŠ¡

* `nginx-proxy`åªéœ€è¦åœ¨workerèŠ‚ç‚¹éƒ¨ç½²ï¼ˆå³åªéœ€è¦åœ¨æ²¡æœ‰`apiserver `çš„èŠ‚ç‚¹éƒ¨ç½²ï¼‰

```bash
# å®šä¹‰Master IPåˆ—è¡¨
MASTERS=(192.168.48.142 192.168.48.143)

# åˆ›å»ºNginxé…ç½®æ–‡ä»¶ç›®å½•
mkdir -p /etc/nginx

# åˆ›å»ºNginxé…ç½®æ–‡ä»¶(æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ä¸‹æ–¹upstreaméƒ¨åˆ†)
cat >/etc/nginx/nginx.conf <<EOF
error_log stderr notice;

worker_processes 2;
worker_rlimit_nofile 130048;
worker_shutdown_timeout 10s;

events {
  multi_accept on;
  use epoll;
  worker_connections 16384;
}

stream {
  upstream kube_apiserver {
    least_conn;
    server ${MASTERS[0]}:6443;
    server ${MASTERS[1]}:6443;
  }

  server {
    listen        127.0.0.1:6443;
    proxy_pass    kube_apiserver;
    proxy_timeout 10m;
    proxy_connect_timeout 1s;
  }
}

http {
  aio threads;
  aio_write on;
  tcp_nopush on;
  tcp_nodelay on;

  keepalive_timeout 5m;
  keepalive_requests 100;
  reset_timedout_connection on;
  server_tokens off;
  autoindex off;

  server {
    listen 8081;
    location /healthz {
      access_log off;
      return 200;
    }
    location /stub_status {
      stub_status on;
      access_log off;
    }
  }
}
EOF

# åˆ›å»ºProxy Pod
mkdir -p /etc/kubernetes/manifests/

cat >/etc/kubernetes/manifests/nginx-proxy.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-proxy
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: kube-nginx
spec:
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet
  nodeSelector:
    kubernetes.io/os: linux
  priorityClassName: system-node-critical
  containers:
  - name: nginx-proxy
    image: docker.io/library/nginx:1.19
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 25m
        memory: 32M
    securityContext:
      privileged: true
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8081
    readinessProbe:
      httpGet:
        path: /healthz
        port: 8081
    volumeMounts:
    - mountPath: /etc/nginx
      name: etc-nginx
      readOnly: true
  volumes:
  - name: etc-nginx
    hostPath:
      path: /etc/nginx
EOF

# åœ¨æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸‹è½½é•œåƒ
crictl pull registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2
ctr -n k8s.io i tag  registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2 k8s.gcr.io/pause:3.2
```

#### é…ç½®kube-proxy

```bash
# å‡†å¤‡kubeconfigé…ç½®æ–‡ä»¶
mv ~/tmp.node.kubeconfig/kube-proxy.kubeconfig /etc/kubernetes/

# åˆ›å»ºYAML
cat >/etc/kubernetes/kube-proxy-config.yaml <<EOF
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: 0.0.0.0
clientConnection:
  kubeconfig: "/etc/kubernetes/kube-proxy.kubeconfig"
clusterCIDR: "10.200.0.0/16"
mode: ipvs
EOF

# åˆ›å»ºSystem Service
cat >/etc/systemd/system/kube-proxy.service <<EOF
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/etc/kubernetes/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

#### æ¸…ç†ä¸´æ—¶ç›®å½•

```bash
rmdir ~/tmp.node.kubeconfig
```

#### å¯åŠ¨æœåŠ¡

```bash
systemctl daemon-reload

systemctl restart kubelet kube-proxy && systemctl enable kubelet kube-proxy

systemctl status kubelet && systemctl status kube-proxy

journalctl -f -u kubelet
journalctl -f -u kube-proxy
```

### éƒ¨ç½²ç½‘ç»œæ’ä»¶Calico

æ–‡æ¡£ï¼š[https://projectcalico.docs.tigera.io/getting-started/kubernetes/self-managed-onprem/onpremises](https://projectcalico.docs.tigera.io/getting-started/kubernetes/self-managed-onprem/onpremises)

ï¼ˆ1ï¼‰ä¸‹è½½YAMLæ–‡ä»¶

```bash
curl https://projectcalico.docs.tigera.io/manifests/calico.yaml -O
```

ï¼ˆ2ï¼‰ä¿®æ”¹IPè‡ªåŠ¨å‘ç°

å½“kubeletçš„å¯åŠ¨å‚æ•°ä¸­å­˜åœ¨`--node-ip`çš„æ—¶å€™ï¼Œä»¥host-networkæ¨¡å¼å¯åŠ¨çš„podçš„status.hostIPå­—æ®µå°±ä¼šè‡ªåŠ¨å¡«å…¥kubeletä¸­æŒ‡å®šçš„ipåœ°å€

```bash
# ä¿®æ”¹å‰
- name: IP
  value: "autodetect"
  
# ä¿®æ”¹å
- name: IP
  valueFrom:
    fieldRef:
      fieldPath: status.hostIP
```

ï¼ˆ3ï¼‰ä¿®æ”¹CIDR

```bash
# ä¿®æ”¹å‰
# - name: CALICO_IPV4POOL_CIDR
#   value: "192.168.0.0/16"

# ä¿®æ”¹å
- name: CALICO_IPV4POOL_CIDR
  value: "10.200.0.0/16"
```

ï¼ˆ4ï¼‰éƒ¨ç½²

```bash
[root@node-1 ~]# kubectl apply -f calico.yaml 
```

ï¼ˆ5ï¼‰æ£€æŸ¥çŠ¶æ€

```bash
# æ£€æŸ¥PodçŠ¶æ€
[root@node-1 ~]# kubectl get pods -A 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-5b97f5d8cf-nmx9v   1/1     Running   0          11m
kube-system   calico-node-djmlg                          1/1     Running   0          11m
kube-system   calico-node-ph5lj                          1/1     Running   0          11m
kube-system   calico-node-wshgm                          1/1     Running   0          11m
kube-system   nginx-proxy-node-3                         1/1     Running   0          29m

# æŸ¥çœ‹NodeçŠ¶æ€ï¼Œå·²ç»å˜æˆReadyäº†
[root@node-1 ~]# kubectl get node
NAME     STATUS   ROLES    AGE   VERSION
node-1   Ready    <none>   40m   v1.24.4
node-2   Ready    <none>   40m   v1.24.4
node-3   Ready    <none>   29m   v1.24.4
```

### éƒ¨ç½²DNSæ’ä»¶CoreDNS

æ–‡æ¡£ï¼š

* corednså®˜æ–¹æ–‡æ¡£ï¼š[https://coredns.io/plugins/kubernetes/](https://coredns.io/plugins/kubernetes/)
* NodeLocal DNSCacheï¼š[https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/)

ï¼ˆ1ï¼‰éƒ¨ç½²coredns

```bash
# ä¸‹è½½coredns yaml
[root@node-1 ~]# wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed
[root@node-1 ~]# wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.sh
[root@node-1 ~]# chmod +x deploy.sh
[root@node-1 ~]# ./deploy.sh -i 10.233.0.10 > coredns.yaml

# éƒ¨ç½²
[root@node-1 ~]# kubectl apply -f coredns.yaml
```

ï¼ˆ2ï¼‰éƒ¨ç½²NodeLocal DNSCache

æ–‡æ¡£ï¼š[https://github.com/kubernetes/kubernetes/tree/v1.24.3/cluster/addons/dns/nodelocaldns](https://github.com/kubernetes/kubernetes/tree/v1.24.3/cluster/addons/dns/nodelocaldns)

```bash
# æ‹·è´yamlæ–‡ä»¶
cp ~/kubernetes/src/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml .

# è®¾ç½®ä¸º kube-dns service ip,è¿™é‡Œå¹¶æ²¡æœ‰ç”¨åˆ°kube-dnsï¼Œæ‰€ä»¥ç½®ä¸ºç©º
sed -ri 's/,__PILLAR__DNS__SERVER__//g' nodelocaldns.yaml
sed -ri 's/__PILLAR__DNS__SERVER__//g' nodelocaldns.yaml

# è®¾ç½®ä¸ºæœ¬åœ°é“¾æ¥IPï¼Œè¿™ä¸ªå€¼è¦å’Œkubeleté…ç½®ä¸­çš„clusterDNSç›¸åŒ
sed -ri 's/__PILLAR__LOCAL__DNS__/169.254.25.10/g' nodelocaldns.yaml

# è®¾ç½®DNSåŸŸååœ°å€ï¼Œé»˜è®¤ä¸ºcluster.local
sed -ri 's/__PILLAR__DNS__DOMAIN__/cluster.local/g' nodelocaldns.yaml

# è®¾ç½®é›†ç¾¤å†…éƒ¨æŸ¥è¯¢çš„ä¸Šæ¸¸æœåŠ¡å™¨ï¼Œè¿™ä¸ªå€¼å’Œcorednså€¼ä¿æŒä¸€è‡´
sed -ri 's/__PILLAR__CLUSTER__DNS__/10.233.0.10/g' nodelocaldns.yaml

# è®¾ç½®é›†ç¾¤å¤–éƒ¨æŸ¥è¯¢çš„ä¸Šæ¸¸æœåŠ¡å™¨
sed -ri 's#__PILLAR__UPSTREAM__SERVERS__#/etc/resolv.conf#g' nodelocaldns.yaml

# ä½¿ç”¨ç§‘å­¦ä¸Šç½‘æå‰ä¸‹è½½é•œåƒ
[root@node-1 ~]# grep image nodelocaldns.yaml 
        image: k8s.gcr.io/dns/k8s-dns-node-cache:1.21.1

# éƒ¨ç½²
[root@node-1 ~]# kubectl apply -f nodelocaldns.yaml 

# æŸ¥çœ‹Pod
[root@node-1 ~]# kubectl get pods -A | grep node-local-dns
kube-system   node-local-dns-8wqmd                       1/1     Running   0          12s
kube-system   node-local-dns-wdgkw                       1/1     Running   0          12s
kube-system   node-local-dns-z76pz                       1/1     Running   0          12s
```
